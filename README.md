<p align="center">

  <h1 align="center">Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction</h1>
  <p align="center">
    <a href="http://shawlyu.github.io/">Xiaoyang Lyu</a>
    路
    <a href="">Chirui Chang</a>
    路
    <a href="https://daipengwa.github.io/">Peng Dai</a>
    路
    <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a>
    路
    <a href="https://xjqi.github.io/">Xiaojuan Qi</a> 
  </p>
  <h3 align="center">CVPR 2024</h3>
  <h3 align="center"><a href="https://arxiv.org/pdf/2403.19314.pdf">Paper</a> | <a href="">Project Page(Coming Soom)</a></h3>
  <div align="center"></div>
</p>

<p align="center">
  <a href="">
    <img src="./media/teaser.png" alt="Logo" width="95%">
  </a>
</p>

<p align="left">
<strong>TL; DR:</strong> Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics. Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D geometry of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions. In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction. Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition. Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition. We extensively evaluate our method on benchmark datasets and demonstrate its potential for downstream applications, such as animation and scene editing. 
</p>
<br>

# TODO
- [ ] Create the project page
- [ ] Opensource all the training code
- [ ] Opensource the GUI
- [ ] Downstream appliaction
